<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>X Wallet Liveliness</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <!-- Query Parameter Check -->
  <script>
    (function() {
      const urlParams = new URLSearchParams(window.location.search);
      const id = urlParams.get('id');
      // Regular expression for a UUID (v4/v1)
      const uuidRegex = /^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/i;
      if (!id || !uuidRegex.test(id)) {
        alert("Access denied!");
        window.location.href = "about:blank";
      }
    })();
  </script>
  
  <!-- jQuery -->
  <script src="https://code.jquery.com/jquery-3.3.1.min.js"></script>
  
  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  
  <!-- TensorFlow.js Dependencies -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core@2.6.0/dist/tf-core.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter@2.6.0/dist/tf-converter.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm@2.6.0/dist/tf-backend-wasm.min.js"></script>
  
  <!-- Face Landmarks Detection Model -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection@0.0.2/dist/face-landmarks-detection.min.js"></script>
  
  <!-- Webcam Easy -->
  <script src="https://unpkg.com/webcam-easy/dist/webcam-easy.min.js"></script>
  
  <style>
    /* ----- Base Styles ----- */
    body {
      margin: 0;
      padding: 20px;
      display: flex;
      flex-direction: column;
      align-items: center;
      background-color: #f0f0f0;
      font-family: Arial, sans-serif;
      text-align: center;
    }
    h1 {
      margin-bottom: 10px;
    }
    #timer {
      font-size: 18px;
      margin-bottom: 15px;
    }
    #face-guidance {
      font-size: 16px;
      margin-bottom: 15px;
      color: #333;
    }
    #instruction-text {
      font-size: 18px;
      margin-top: 15px;
      color: #333;
    }
    /* ----- Circular UI Styles ----- */
    .circle-container {
      position: relative;
      width: 300px;
      height: 300px;
      border-radius: 50%;
      background-color: #fff;
      box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
      display: flex;
      justify-content: center;
      align-items: center;
      overflow: hidden;
    }
    .circle-border {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      border-radius: 50%;
      border: 5px solid transparent;
      border-top: 5px solid #7e57c2;
      animation: rotate 2s linear infinite;
    }
    .circle-content {
      position: relative;
      width: 90%;
      height: 90%;
      border-radius: 50%;
      background-color: #f8f8f8;
      overflow: hidden;
      display: flex;
      justify-content: center;
      align-items: center;
    }
    /* Set video dimensions for lower resolution (helpful on mobile) */
    .circle-content video {
      width: 640px;
      height: 480px;
      border-radius: 50%;
      transform: scaleX(-1); /* Mirror the video */
    }
    .close-button {
      position: absolute;
      top: 10px;
      right: 10px;
      background-color: #ff6666;
      color: white;
      border: none;
      border-radius: 50%;
      width: 30px;
      height: 30px;
      font-size: 16px;
      display: flex;
      justify-content: center;
      align-items: center;
      cursor: pointer;
    }
    @keyframes rotate {
      0% { transform: rotate(0deg); }
      100% { transform: rotate(360deg); }
    }
    /* ----- Hidden Elements for Internal Drawing ----- */
    #bounding-box, #facial-landmarks {
      display: none;
    }
    /* Download button: initially hidden */
    #download-photo {
      display: none;
      margin-top: 15px;
      padding: 8px 15px;
      background: #7e57c2;
      color: #fff;
      text-decoration: none;
      border-radius: 4px;
    }
    /* Style for the Start button */
    #startBtn {
      top: 10px;
      right: 10px;
      padding: 10px 15px;
      font-size: 16px;
      z-index: 1000;
      background-color: #7e57c2;
      color: #fff;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
  </style>
  
  <!-- Global Audio Unlocking -->
  <script>
    // Create a global Audio object for the "ting" sound.
    const tingAudio = new Audio('res/ting.wav');
    tingAudio.volume = 0.5;
    
    document.addEventListener('DOMContentLoaded', () => {
      document.getElementById('startBtn').addEventListener('click', function() {
        tingAudio.play().then(() => {
          console.log("Audio unlocked");
        }).catch(error => {
          console.error("Error unlocking audio:", error);
        });
        startWebcamAndCapture();
        this.style.display = 'none';
      });
    });
    
    function playTingSound() {
      tingAudio.currentTime = 0;
      tingAudio.play().catch(error => console.error("Error playing sound:", error));
    }
  </script>
</head>
<body>
  <!-- Start Button for unlocking audio on mobile -->
  <button id="startBtn">Start</button>
  
  <!-- Heading -->
  <h1>XWallet Liveliness</h1>
  
  <!-- Instruction Text -->
  <div id="face-guidance">Align your face with the circle and follow instructions.</div>
  
  <!-- Circular UI Container with Video -->
  <div class="circle-container">
    <div class="circle-border"></div>
    <div class="circle-content">
      <video id="webcam" width="640" height="480" autoplay playsinline muted></video>
    </div>
    <button class="close-button" onclick="stopCapture()">Ã—</button>
  </div>
  
  <!-- Instruction text -->
  <div id="instruction-text">When you are ready, click the Start button.</div>
  
  <!-- Hidden canvases for internal drawing -->
  <canvas id="bounding-box" width="640" height="480"></canvas>
  <canvas id="facial-landmarks" width="640" height="480"></canvas>
  
  <!-- Hidden download link for the captured snapshot -->
  <a id="download-photo" href="#" download="snapshot.png">Download Snapshot</a>
  
  <script>
    // ----- Global Variables & Initialization -----
    const webcamElement = document.getElementById('webcam');
    const boundingBoxCanvas = document.getElementById('bounding-box');
    const boundingBoxCtx = boundingBoxCanvas.getContext('2d');
    const landmarksCanvas = document.getElementById('facial-landmarks');
    const landmarksCtx = landmarksCanvas.getContext('2d');
  
    const instructionText = $("#instruction-text");
    const downloadPhotoLink = $("#download-photo");
  
    // Initialize Webcam Easy with lower resolution constraints.
    let webcam = new Webcam(webcamElement, 'user', landmarksCanvas, { width: 640, height: 480 });
    let model = null;
    let running = false;
    let cameraFrame = null;
    let frameCounter = 0; // For throttling detection
  
    // Application states:
    // "waitingForBlink" â†’ "waitingForHeadMovements" â†’ "waitingForCenter" â†’ "waitingForSmile" â†’ "completed"
    let currentStep = 'waitingForBlink';
  
    // Head movement directions and sequence.
    const headMovements = ['left', 'right', 'up', 'down'];
    let headMovementSequence = [];
    let currentHeadMovementIndex = 0;
  
    // Emoji mapping for instructions.
    const emojiMapping = {
      blink: 'ðŸ‘€',
      smile: 'ðŸ˜Š',
      left: 'â¬…ï¸',
      right: 'âž¡ï¸',
      up: 'â¬†ï¸',
      down: 'â¬‡ï¸'
    };
  
    // ----- Setup and Starting Functions -----
    async function setupFaceLandmarkDetection() {
      // Force mobile devices to use the WebGL backend.
      let backend = 'wasm';
      if (/Mobi|Android/i.test(navigator.userAgent)) {
        backend = 'webgl';
      }
      console.log("Setting backend to:", backend);
      await tf.setBackend(backend);
      await tf.ready();
      console.log(`TensorFlow.js backend '${tf.getBackend()}' is ready.`);
    }
  
    async function startWebcamAndCapture() {
      try {
        await webcam.start();
        console.log("Webcam started");
        startCapture();
      } catch (err) {
        console.error("Error starting webcam:", err);
        alert("Could not access the webcam. Please allow camera access.");
      }
    }
  
    async function startCapture() {
      currentStep = 'waitingForBlink';
      headMovementSequence = [];
      currentHeadMovementIndex = 0;
      downloadPhotoLink.hide();
  
      // Ensure the backend is ready before loading the model.
      await tf.ready();
      try {
        model = await faceLandmarksDetection.load(
          faceLandmarksDetection.SupportedPackages.mediapipeFacemesh,
          { maxFaces: 1 }
        );
        console.log("Model loaded successfully.");
        running = true;
        cameraFrame = requestAnimationFrame(detectKeyPoints);
      } catch (err) {
        console.error("Error loading the model:", err);
        alert("Failed to load the face detection model.");
        stopCapture();
      }
    }
  
    function stopCapture() {
      if (running) {
        running = false;
        if (cameraFrame != null) {
          cancelAnimationFrame(cameraFrame);
        }
        boundingBoxCtx.clearRect(0, 0, boundingBoxCanvas.width, boundingBoxCanvas.height);
        landmarksCtx.clearRect(0, 0, landmarksCanvas.width, landmarksCanvas.height);
      }
    }
  
    function stopWebcam() {
      webcam.stop();
      instructionText.text("Webcam stopped.");
    }
  
    // ----- Detection & Drawing Functions -----
    async function detectKeyPoints() {
      if (!running) return;
      
      // Throttle detection to every 2 frames to reduce load on mobile.
      frameCounter++;
      if (frameCounter % 2 !== 0) {
        cameraFrame = requestAnimationFrame(detectKeyPoints);
        return;
      }
  
      const predictions = await model.estimateFaces({
        input: webcamElement,
        returnTensors: false,
        flipHorizontal: true,
        predictIrises: false
      });
  
      boundingBoxCtx.clearRect(0, 0, boundingBoxCanvas.width, boundingBoxCanvas.height);
      landmarksCtx.clearRect(0, 0, landmarksCanvas.width, landmarksCanvas.height);
  
      if (predictions.length > 0) {
        const prediction = predictions[0];
        const keypoints = prediction.scaledMesh;
  
        // Draw bounding box (for debugging)
        const start = prediction.boundingBox.topLeft;
        const end = prediction.boundingBox.bottomRight;
        const size = [end[0] - start[0], end[1] - start[1]];
        boundingBoxCtx.strokeStyle = "blue";
        boundingBoxCtx.lineWidth = 2;
        boundingBoxCtx.strokeRect(start[0], start[1], size[0], size[1]);
  
        drawFacialLandmarks(keypoints, currentStep);
  
        if (currentStep === 'waitingForHeadMovements') {
          const currentDirection = headMovementSequence[currentHeadMovementIndex];
          drawHeadIndicator(currentDirection);
        }
  
        switch (currentStep) {
          case 'waitingForBlink':
            if (detectBlink(keypoints)) {
              console.log('Blink detected!');
              playTingSound();
              for (let i = 0; i < 7; i++) {
                headMovementSequence.push(headMovements[Math.floor(Math.random() * headMovements.length)]);
              }
              console.log("Head Movement Sequence:", headMovementSequence);
              currentStep = 'waitingForHeadMovements';
              instructionText.text(getCurrentInstruction());
            }
            break;
  
          case 'waitingForHeadMovements': {
              const currentDirection = headMovementSequence[currentHeadMovementIndex];
              if (detectHeadMovement(keypoints, currentDirection)) {
                console.log(`Head movement (${currentDirection}) detected!`);
                playTingSound();
                currentHeadMovementIndex++;
                if (currentHeadMovementIndex < headMovementSequence.length) {
                  instructionText.text(`Good! Next, please move your head ${headMovementSequence[currentHeadMovementIndex]} ${emojiMapping[headMovementSequence[currentHeadMovementIndex]]}`);
                } else {
                  instructionText.text(`All head movements detected! Now, please center your face.`);
                  currentStep = 'waitingForCenter';
                }
              }
            }
            break;
  
          case 'waitingForCenter':
            if (detectHeadMovement(keypoints, 'center')) {
              console.log("Face is centered!");
              playTingSound();
              instructionText.text(`Face centered! Now, please smile ${emojiMapping.smile}`);
              currentStep = 'waitingForSmile';
            }
            break;
  
          case 'waitingForSmile':
            if (detectSmile(keypoints)) {
              console.log('Smile detected!');
              playTingSound();
              instructionText.text('Smile detected! Capture completed.');
              let picture = webcam.snap();
              downloadPhotoLink.attr('href', picture);
              downloadPhotoLink.show();
              instructionText.text('Capture completed! You can download your snapshot.');
              currentStep = 'completed';
              stopCapture();
              webcam.stop();
              webcamElement.style.display = "none";
              console.log("Webcam closed after snapshot capture.");
            }
            break;
  
          default:
            break;
        }
      } else {
        console.log("No face detected.");
      }
  
      cameraFrame = requestAnimationFrame(detectKeyPoints);
    }
  
    function getCurrentInstruction() {
      switch (currentStep) {
        case 'waitingForBlink':
          return `Please blink ${emojiMapping.blink}`;
        case 'waitingForHeadMovements':
          const movement = headMovementSequence[currentHeadMovementIndex];
          return movement ? `Please move your head ${movement} ${emojiMapping[movement]}` : 'Please move your head.';
        case 'waitingForCenter':
          return `Please center your face.`;
        case 'waitingForSmile':
          return `Please smile ${emojiMapping.smile}`;
        default:
          return "";
      }
    }
  
    function detectHeadMovement(keypoints, direction) {
      const noseTipIndex = 1;
      const nose = keypoints[noseTipIndex];
      const noseX = nose[0];
      const noseY = nose[1];
      
      // Use video dimensions if available; otherwise, fall back to canvas dimensions.
      const videoWidth = webcamElement.videoWidth || boundingBoxCanvas.width;
      const videoHeight = webcamElement.videoHeight || boundingBoxCanvas.height;
      const frameCenterX = videoWidth / 2;
      const frameCenterY = videoHeight / 2;
      
      // Set thresholds to 5% of the video dimensions.
      const THRESHOLD_X = videoWidth * 0.05;
      const THRESHOLD_Y = videoHeight * 0.05;
  
      let moved = false;
      switch(direction) {
        case 'left':
          if (noseX < frameCenterX - THRESHOLD_X) moved = true;
          break;
        case 'right':
          if (noseX > frameCenterX + THRESHOLD_X) moved = true;
          break;
        case 'up':
          if (noseY < frameCenterY - THRESHOLD_Y) moved = true;
          break;
        case 'down':
          if (noseY > frameCenterY + THRESHOLD_Y) moved = true;
          break;
        case 'up-left':
          if (noseX < frameCenterX - THRESHOLD_X && noseY < frameCenterY - THRESHOLD_Y) moved = true;
          break;
        case 'up-right':
          if (noseX > frameCenterX + THRESHOLD_X && noseY < frameCenterY - THRESHOLD_Y) moved = true;
          break;
        case 'down-left':
          if (noseX < frameCenterX - THRESHOLD_X && noseY > frameCenterY + THRESHOLD_Y) moved = true;
          break;
        case 'down-right':
          if (noseX > frameCenterX + THRESHOLD_X && noseY > frameCenterY + THRESHOLD_Y) moved = true;
          break;
        case 'center':
          if (Math.abs(noseX - frameCenterX) < THRESHOLD_X && Math.abs(noseY - frameCenterY) < THRESHOLD_Y) {
            moved = true;
          }
          break;
        default:
          break;
      }
      console.log(`Nose: (${noseX.toFixed(1)}, ${noseY.toFixed(1)}) | Frame Center: (${frameCenterX.toFixed(1)}, ${frameCenterY.toFixed(1)}) | THRESHOLD: (${THRESHOLD_X.toFixed(1)}, ${THRESHOLD_Y.toFixed(1)}) | Target: ${direction} | Moved: ${moved}`);
      return moved;
    }
  
    function drawHeadIndicator(direction) {
      landmarksCtx.save();
      landmarksCtx.strokeStyle = "yellow";
      landmarksCtx.fillStyle = "yellow";
      landmarksCtx.lineWidth = 4;
      const centerX = landmarksCanvas.width / 2;
      const centerY = landmarksCanvas.height / 2;
      const arrowLength = 40;
      landmarksCtx.beginPath();
      switch(direction) {
        case 'left':
          landmarksCtx.moveTo(centerX, centerY);
          landmarksCtx.lineTo(centerX - arrowLength, centerY);
          landmarksCtx.lineTo(centerX - arrowLength + 10, centerY - 10);
          landmarksCtx.moveTo(centerX - arrowLength, centerY);
          landmarksCtx.lineTo(centerX - arrowLength + 10, centerY + 10);
          break;
        case 'right':
          landmarksCtx.moveTo(centerX, centerY);
          landmarksCtx.lineTo(centerX + arrowLength, centerY);
          landmarksCtx.lineTo(centerX + arrowLength - 10, centerY - 10);
          landmarksCtx.moveTo(centerX + arrowLength, centerY);
          landmarksCtx.lineTo(centerX + arrowLength - 10, centerY + 10);
          break;
        case 'up':
          landmarksCtx.moveTo(centerX, centerY);
          landmarksCtx.lineTo(centerX, centerY - arrowLength);
          landmarksCtx.lineTo(centerX - 10, centerY - arrowLength + 10);
          landmarksCtx.moveTo(centerX, centerY - arrowLength);
          landmarksCtx.lineTo(centerX + 10, centerY - arrowLength + 10);
          break;
        case 'down':
          landmarksCtx.moveTo(centerX, centerY);
          landmarksCtx.lineTo(centerX, centerY + arrowLength);
          landmarksCtx.lineTo(centerX - 10, centerY + arrowLength - 10);
          landmarksCtx.moveTo(centerX, centerY + arrowLength);
          landmarksCtx.lineTo(centerX + 10, centerY + arrowLength - 10);
          break;
        default:
          break;
      }
      landmarksCtx.stroke();
      landmarksCtx.restore();
    }
  
    let blinkFrameCount = 0;
    function detectBlink(keypoints) {
      const leftEyeIndices = { top: 386, bottom: 374, left: 263, right: 362 };
      const rightEyeIndices = { top: 159, bottom: 145, left: 33, right: 133 };
      const leftEAR = calculateEAR(keypoints, leftEyeIndices);
      const rightEAR = calculateEAR(keypoints, rightEyeIndices);
      console.log(`Left EAR: ${leftEAR.toFixed(3)}\tRight EAR: ${rightEAR.toFixed(3)}`);
      const EAR_THRESHOLD = 0.5;
      const REQUIRED_CONSECUTIVE_FRAMES = 3;
      if (leftEAR < EAR_THRESHOLD && rightEAR < EAR_THRESHOLD) {
        blinkFrameCount++;
      } else {
        blinkFrameCount = 0;
      }
      if (blinkFrameCount >= REQUIRED_CONSECUTIVE_FRAMES) {
        blinkFrameCount = 0;
        return true;
      }
      return false;
    }
  
    // ----- Smile Detection Using Mouth Corner Lift -----
    let smileFrameCount = 0;
    const REQUIRED_CONSECUTIVE_SMILE_FRAMES = 5;
    const SMILE_CORNER_LIFT_THRESHOLD = 0.02;
  
    function detectSmile(keypoints) {
      const leftCorner = keypoints[61];
      const rightCorner = keypoints[291];
      const topLip = keypoints[13];
      const bottomLip = keypoints[14];
  
      const mouthCenterX = (topLip[0] + bottomLip[0]) / 2;
      const mouthCenterY = (topLip[1] + bottomLip[1]) / 2;
  
      const horizontalDist = euclideanDistance(
        leftCorner[0], leftCorner[1],
        rightCorner[0], rightCorner[1]
      );
  
      const leftLift = mouthCenterY - leftCorner[1];
      const rightLift = mouthCenterY - rightCorner[1];
      const avgNormalizedLift = ((leftLift + rightLift) / 2) / horizontalDist;
      
      console.log(`Average Normalized Corner Lift: ${avgNormalizedLift.toFixed(3)}`);
  
      if (avgNormalizedLift > SMILE_CORNER_LIFT_THRESHOLD) {
        smileFrameCount++;
      } else {
        smileFrameCount = 0;
      }
  
      if (smileFrameCount >= REQUIRED_CONSECUTIVE_SMILE_FRAMES) {
        smileFrameCount = 0;
        return true;
      }
  
      return false;
    }
  
    function drawFacialLandmarks(keypoints, step) {
      landmarksCtx.lineWidth = 2;
      landmarksCtx.strokeStyle = "white";
      landmarksCtx.fillStyle = "white";
      const eyes = {
        left: [33, 7, 163, 144, 145, 153, 154, 155, 133, 33],
        right: [263, 249, 390, 373, 374, 380, 381, 382, 362, 263]
      };
      const mouth = {
        outer: [61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291, 308, 324, 318, 402, 317, 14, 87, 178, 88, 95, 185, 40, 39, 37, 0, 267, 269, 270, 409, 415, 310, 311, 312, 13, 82, 81, 80, 191, 78, 95]
      };
      function drawLines(landmarkIndices) {
        landmarksCtx.beginPath();
        for (let i = 0; i < landmarkIndices.length; i++) {
          const [x, y] = keypoints[landmarkIndices[i]];
          if (i === 0) {
            landmarksCtx.moveTo(x, y);
          } else {
            landmarksCtx.lineTo(x, y);
          }
        }
        landmarksCtx.stroke();
      }
      drawLines(eyes.left);
      drawLines(eyes.right);
      if (step === 'waitingForHeadMovements') {
        const faceOutline = [10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288, 397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136, 172, 58, 132, 93, 234, 127, 162, 21, 54, 103, 67, 109, 10];
        drawLines(faceOutline);
      }
      if (step === 'waitingForSmile') {
        drawLines(mouth.outer);
      }
    }
  
    function calculateEAR(keypoints, eyeIndices) {
      const a = euclideanDistance(
        keypoints[eyeIndices.top][0], keypoints[eyeIndices.top][1],
        keypoints[eyeIndices.bottom][0], keypoints[eyeIndices.bottom][1]
      );
      const b = euclideanDistance(
        keypoints[eyeIndices.left][0], keypoints[eyeIndices.left][1],
        keypoints[eyeIndices.right][0], keypoints[eyeIndices.right][1]
      );
      return a / (2.0 * b);
    }
  
    function euclideanDistance(x1, y1, x2, y2) {
      return Math.sqrt(Math.pow((x1 - x2), 2) + Math.pow((y1 - y2), 2));
    }
  
    // ----- Start-up -----
    async function main() {
      await setupFaceLandmarkDetection();
      // Uncomment the next line to auto-start detection:
      // startWebcamAndCapture();
    }
    main();
  </script>
</body>
</html>
